---
id: case_studies
title: Case Studies
sidebar_label: Case Studies
---

> Some lessons from articles may end up in `Study` or `Tools` sections.

## Unread

- [Taking Back What Is Already Yours: Router Wars Episode I](https://0x90.psaux.io/2020/03/01/Taking-Back-What-Is-Already-Yours-Router-Wars-Episode-I/)
- [Consistent Hashing in viemo](https://medium.com/vimeo-engineering-blog/improving-load-balancing-with-a-new-consistent-hashing-algorithm-9f1bd75709ed)
- [The Mystery of the Slow Downloads](https://panic.com/blog/mystery-of-the-slow-downloads/)
- [What does an idle CPU do?](https://manybutfinite.com/post/what-does-an-idle-cpu-do/)
- [How to store data forever](https://drewdevault.com/2020/04/22/How-to-store-data-forever.html)
- [SRE: Debugging: Simple Memory Leaks in Go](https://medium.com/dm03514-tech-blog/sre-debugging-simple-memory-leaks-in-go-e0a9e6d63d4d)
- [Why I Prefer systemd Timers Over Cron](https://trstringer.com/systemd-timer-vs-cronjob/)
- [ üåü How to trigger races reliably](https://people.kernel.org/metan/how-to-trigger-races-reliably)
- [Why Not WireGuard](https://blog.ipfire.org/post/why-not-wireguard)
- [Why not "Why not WireGuard?"](https://tailscale.com/blog/why-not-why-not-wireguard/)
- [Intel Virtualisation: How VT-x, KVM and QEMU Work Together](https://binarydebt.wordpress.com/2018/10/14/intel-virtualisation-how-vt-x-kvm-and-qemu-work-together/)
- [Implementing Stripe-like Idempotency Keys in Postgres](https://brandur.org/idempotency-keys)
- [ üåü Time on Unix](https://venam.nixers.net/blog/unix/2020/05/02/time-on-unix.html)
- [SSH Tunnel - Local, Remote and Dynamic Port Forwarding](https://blog.jakuba.net/ssh-tunnel---local-remote-and-dynamic-port-forwarding/)
- [SSH Tips & Tricks](https://smallstep.com/blog/ssh-tricks-and-tips/)
- [Anybody can write good bash (with a little effort)](https://blog.yossarian.net/2020/01/23/Anybody-can-write-good-bash-with-a-little-effort)
- [Can QUIC match TCP‚Äôs computational efficiency?](https://www.fastly.com/blog/measuring-quic-vs-tcp-computational-efficiency)
- [eBPF - Rethinking the Linux Kernel](https://docs.google.com/presentation/d/1AcB4x7JCWET0ysDr0gsX-EIdQSTyBtmi6OAW7bE0jm0/mobilepresent#slide=id.g35f391192_00)
- [Lessons from using eBPF (and bypassing TCP/IP) for accelerating Cloud Native applications](https://cyral.com/blog/lessons-using-ebpf-accelerating-cloud-native)
- [Debugging Distributed Systems](https://dl.acm.org/doi/pdf/10.1145/2927299.2940294)
- [Packaging LXD for Arch Linux](https://linderud.dev/blog/packaging-lxd-for-arch-linux/)
- [Inside the messy mission to bring 4G to the London Underground](https://www.wired.co.uk/article/london-underground-4g)
- [Hunting a Linux kernel bug](https://blog.twitter.com/engineering/en_us/topics/open-source/2020/hunting-a-linux-kernel-bug.html)
- [Building a WireGuard Jail with the FreeBSD's Standard Tools](https://genneko.github.io/playing-with-bsd/networking/freebsd-wireguard-jail/)
- [Git Internals](https://www.chromium.org/developers/fast-intro-to-git-internals)
- [How io_uring and eBPF Will Revolutionize Programming in Linux](https://thenewstack.io/how-io_uring-and-ebpf-will-revolutionize-programming-in-linux/)
- [etcd maintenance](https://blog.gojekengineering.com/a-few-notes-on-etcd-maintenance-c06440011cbe)
- [Save Your Linux Machine From Certain Death](https://medium.com/better-programming/save-your-linux-machine-from-certain-death-24ced335d969)
- [ How are docker images built? A look into the Linux overlay file-systems and the OCI specification](https://dev.to/napicella/how-are-docker-images-built-a-look-into-the-linux-overlay-file-systems-and-the-oci-specification-175n)
- [Dockerless, part 1: Which tools to replace Docker with and why](https://mkdev.me/en/posts/dockerless-part-1-which-tools-to-replace-docker-with-and-why)
- [WebSocket Load Testing with Artillery.io](https://itnext.io/websocket-load-testing-with-artillery-io-b8b7ecbcd7ed)
- [Why strace doesn't work in Docker](https://jvns.ca/blog/2020/04/29/why-strace-doesnt-work-in-docker/)
- [WireGuard on K8s (road-warrior-style VPN server)](https://blog.levine.sh/14058/wireguard-on-k8s-road-warrior-style-vpn-server)
- [Exploring Kernel Networking: BPF Hook Points, Part 1 ‚Äì Elementary, My Dear Watson](https://blog.stackpath.com/bpf-hook-points-part-1/)
- [Context propagation over HTTP in Go](https://medium.com/@rakyll/context-propagation-over-http-in-go-d4540996e9b0)
- [Why we use the Linux kernel's TCP stack](https://blog.cloudflare.com/why-we-use-the-linux-kernels-tcp-stack/)
- [Caching at Reddit](https://redditblog.com/2017/01/17/caching-at-reddit/)
- [The Sad Story of TCP Fast Open](https://squeeze.isobar.com/2019/04/11/the-sad-story-of-tcp-fast-open/)
- [Linux Netfilter: How does connection tracking track connections changed by NAT?](https://superuser.com/questions/1269859/linux-netfilter-how-does-connection-tracking-track-connections-changed-by-nat)
- [When Linux conntrack is no longer your friend](https://www.projectcalico.org/when-linux-conntrack-is-no-longer-your-friend/)
- [The network nightmare that ate my week](https://blog.bimajority.org/2014/09/05/the-network-nightmare-that-ate-my-week/)
- [State of caching for Go](https://dgraph.io/blog/post/caching-in-go/)
- [Setting the Record Straight: containers vs. Zones vs. Jails vs. VMs](https://blog.jessfraz.com/post/containers-zones-jails-vms/)
- [‚ÄúC is how the computer works‚Äù is a dangerous mindset for C programmers](https://words.steveklabnik.com/c-is-how-the-computer-works-is-a-dangerous-mindset-for-c-programmers)
- [da memory hierarchy](http://www.pixelbeat.org/docs/memory_hierarchy/)
- [Systems Benchmarking Crimes](https://www.cse.unsw.edu.au/~gernot/benchmarking-crimes.html)
- [RSS: The Original Federated Social Network Protocol](https://battlepenguin.com/tech/rss-the-original-federated-social-network-protocol/)
- [Why you shouldn't use func main in Go.](https://pace.dev/blog/2020/02/12/why-you-shouldnt-use-func-main-in-golang-by-mat-ryer)
- [WireGuide: All about the WireGuard VPN protocol](https://www.privateinternetaccess.com/blog/wireguide-all-about-the-wireguard-vpn-protocol/)
- [Challenges with distributed systems](https://aws.amazon.com/builders-library/challenges-with-distributed-systems/)
- [Measuring context switching and memory overheads for Linux threads](https://eli.thegreenplace.net/2018/measuring-context-switching-and-memory-overheads-for-linux-threads/)
- [CRDs Killed the Free Kubernetes Control Plane](https://caleblloyd.com/software/crds-killed-free-kubernetes-control-plane/)
- [Becoming a Git pro. Part 1: internal Git architecture](https://indepth.dev/becoming-a-git-pro-part-1-internal-git-architecture/)
- [Regular Expression Matching Can Be Simple And Fast](https://swtch.com/~rsc/regexp/regexp1.html)
- [The Effect of Pipe Capacity on Unix Pipeline Performance](http://dongyuxuan.me/posts/pipeline.html)
- [Go To Statement Considered Harmful](http://www.u.arizona.edu/~rubinson/copyright_violations/Go_To_Considered_Harmful.html)
- [Path MTU discovery in practice](https://blog.cloudflare.com/path-mtu-discovery-in-practice/)
- [How 1500 bytes became the MTU of the internet](https://blog.benjojo.co.uk/post/why-is-ethernet-mtu-1500)
- [Understanding the bin, sbin, usr/bin , usr/sbin split](http://lists.busybox.net/pipermail/busybox/2010-December/074114.html)
- [The death of a TLD](https://blog.benjojo.co.uk/post/the-death-of-a-tld)
- [Writing userspace USB drivers for abandoned devices](https://blog.benjojo.co.uk/post/userspace-usb-drivers)
- [Just how long do DNS resolvers cache last?](https://blog.benjojo.co.uk/post/dns-resolvers-ttl-lasts-over-one-week)
- [Monitoring SNMP less devices with ease](https://blog.benjojo.co.uk/post/monitoring-wifi-devices-without-snmp)
- [Building a legacy search engine for a legacy protocol](https://blog.benjojo.co.uk/post/building-a-search-engine-for-gopher)
- [Detecting anycast addresses and more](https://blog.benjojo.co.uk/post/path-detection-for-anycast)
- [The strange case of ICMP Type 69 on Linux](https://blog.benjojo.co.uk/post/linux-icmp-type-69)
- [I may be the only evil (bit) user on the internet](https://blog.benjojo.co.uk/post/evil-bit-RFC3514-real-world-usage)
- [Dealing with MySQL resource shortage](https://blog.benjojo.co.uk/post/2014-04-17-mysql-resource-shortage.md)
- [MITM‚Äôing TLS/SSL for debugging purposes](https://blog.benjojo.co.uk/post/debug-ssl-tls-with-ssldump-https)
- [Making HTTP Requests with Telnet](https://doesnotscale.com/making-http-requests-with-telnet/)
- [Announcing GitTorrent: A Decentralized GitHub](https://blog.printf.net/articles/2015/05/29/announcing-gittorrent-a-decentralized-github/)
- [How many IP addresses can a DNS query return?](https://ethanheilman.tumblr.com/post/110920218915/how-many-ip-addresses-can-a-dns-query-return)
- [shebangs and busybox](http://xn--rpa.cc/irl/shebang.html)
- [Debugging MySQL replication lag by diving into the internals](https://making.pusher.com/debugging-mysql-replication-lag/)
- [Using select(2)](http://aivarsk.com/2017/04/06/select/)
- [fork() can fail: this is important](http://rachelbythebay.com/w/2014/08/19/fork/)
- [Escape from System D](https://davmac.wordpress.com/2017/06/14/escape-from-system-d/)
- [Why I dislike systemd](http://www.steven-mcdonald.id.au/articles/systemd.shtml)
- [The problem with thread^W event loops](https://blog.cloudflare.com/the-problem-with-event-loops/)
- [Best Practices for ACME Client Operations](https://docs.https.dev/acme-ops)
- [Why turning on HTTTP/2 was a mistake](https://www.lucidchart.com/techblog/2019/04/10/why-turning-on-http2-was-a-mistake/)
- [How fast do I talk](https://www.hillelwayne.com/post/talk-fast/)
- [Why your software should use UUIDs in 2020s](https://devforth.io/blog/why-your-software-should-use-uuids-in-2020s)
- [How to get a core dump for a segfault on Linux](https://jvns.ca/blog/2018/04/28/debugging-a-segfault-on-linux/)
- [Taming Floating Point Error](http://www.johnbcoughlin.com/posts/floating-point-axiom/)
- [Decentralized proofs](https://metacode.biz/openpgp/proofs)
- [A Survey of CPU Caches](https://meribold.org/2017/10/20/survey-of-cpu-caches/)
- [Linux Load Averages: Solving the Mystery](http://www.brendangregg.com/blog/2017-08-08/linux-load-averages.html)
- [SDN Internet Router ‚Äì Part 1](https://labs.spotify.com/2016/01/26/sdn-internet-router-part-1/)
- [Fixing the internet for real time applications](https://technology.riotgames.com/news/fixing-internet-real-time-applications-part-i)
- [mmproxy - Creative Linux routing to preserve client IP addresses in L7 proxies](https://blog.cloudflare.com/mmproxy-creative-way-of-preserving-client-ips-in-spectrum/)
- [Traceroute Lies!](http://movingpackets.net/2017/10/06/misinterpreting-traceroute/)
- [Consistent Hashing google](https://research.googleblog.com/2017/04/consistent-hashing-with-bounded-loads.html)
- [Text Processing VS Hadoop](https://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html)
- [Learn redis the hard way trivago](http://tech.trivago.com/2017/01/25/learn-redis-the-hard-way-in-production/)
- [Fastest Site in the world](https://hackernoon.com/10-things-i-learned-making-the-fastest-site-in-the-world-18a0e1cdf4a7)
- [Crawling 40 Billion Pages](http://www.michaelnielsen.org/ddi/how-to-crawl-a-quarter-billion-webpages-in-40-hours/)
- [Stop using gzip!](http://imoverclocked.blogspot.in/2015/12/for-love-of-bits-stop-using-gzip.html)
- [Adventures in /usr/bin](https://ablagoev.github.io/linux/adventures/commands/2017/02/19/adventures-in-usr-bin.html)
- [Why nix](https://yakking.branchable.com/posts/what-and-why-nix/)
- [Memcached-Backed Content Infrastructure Khanacademy](http://engineering.khanacademy.org/posts/memcached-fms.htm)
- [Dropbox Optimize for web](https://blogs.dropbox.com/tech/2017/09/optimizing-web-servers-for-high-throughput-and-low-latency/)
- [Dumbsmash Scalling with 3 engg](https://stackshare.io/dubsmash/dubsmash-scaling-to-200-million-users-with-3-engineers)
- [Don't parse output of ls](http://mywiki.wooledge.org/ParsingLs)

---

## Technical

### On concurrency in Go HTTP servers

üîç [Link to post](https://eli.thegreenplace.net/2019/on-concurrency-in-go-http-servers/)

- Exploring `net/http`, it employs concurrency; while this is great for high loads, uses ApacheBench to simulate many concurrent connections.
- The initial `inc` method allowed concurrent mutation of a `map` which leads to a race condition. Solved it by using `sync.Mutex` and using a pointer reciever on the `inc` method.
- The same can be done by using channels but is an overkill for simple cases.
  > **Question:** Won't this also lock on `get` and `set`, `set` is fine for locking but `get`?
- Rate limiting, Restricting the number of simultaneous connections or [restricting the number of connections per unit of time](https://gobyexample.com/rate-limiting). One way to do the first one is by using buffered channels to construct semaphores.
- `close(someChan)` indicates that no more values will be sent on `someChan`.

### So you want to expose Go on the Internet

üîç [Link to post](https://blog.cloudflare.com/exposing-go-on-the-internet/)

- Earlier it was general wisdom to always put Go servers behind a reverse proxy like NGINX.
- Good idea to use _Timeouts_ when dealing with untrusted clients. `ReadTimeout`, `WriteTimeout` and `IdleTimeout` can be set in `http.Server`.
- `HTTP/2` is enabled automatically on any Go 1.6+ server if using TLS/HTTPS and it can also be set using various other ways.
- Avoid using `http.DefaultServeMux`; any package you import can have access to it, eg. if anything imports `net/http/pprof`, clients will be able to get CPU profiles. Instead instantiate an `http.ServeMux` yourself and set it as the `Server.Handler`.
- A metric you'll want to monitor is the number of open file descriptors when dealing with webservers. One can use `Server.ConnState` hook to get more detailed metrics of what stage the connections are in.

### Why do we use the Linux kernel's TCP stack?

üîç This is a [combination](https://blog.cloudflare.com/why-we-use-the-linux-kernels-tcp-stack/) of [this](https://jvns.ca/blog/2016/06/30/why-do-we-use-the-linux-kernels-tcp-stack/) and [this](https://www.reddit.com/r/linuxadmin/comments/bnwcan/iptables_vs_nftables/).

We can run programs without having an OS/Kernel, we mostly tend to use an OS because it makes it easy to interact with the hardware using its APIs, also it adds a time sharing layer which allows multiple processes to run. _This is no different for the networking stack._ So it's possible to run a userspace program that'll directly access the network hardware but at the cost of losing the ability to run multiple network applications, that means a single application will be using the network hardware; common term for this is **kernel bypass.**

For **embedded systems**, some people either used [lwIP](https://savannah.nongnu.org/projects/lwip/) or write their own TCP stack to meet their requirements. Another reason to have userspace networking stack will be to add additional networking features when you can't control the underlying kernel version, like Google does with Android vendors. (unsure)

Another reason to use a custom stack would be for **latency and high performance**. Linux Network stack imo does pretty well here, roughly iptables can handle 1Mpps, but if an attack sends 3Mpps then the server will not be able to handle it. Kernel Bypass makes sense for switches or routers, dedicated loadbalancers, high throughput / low latency applications.

If [multiqueue is supported](https://lwn.net/Articles/289137/) instead of doing full kernel bypass we can do partial bypass in which kernel retains ownership of the network hardware and we do the bypass only on one `RX queue`.

> You can see if you have **multiqueue** using `ethtool -l <interface_name>`, if it shows non found or something similar, that means you either don't have multiqueue or the NIC driver is not correctly setup.

An example of a kernel bypass technology here would be [DPDK](https://en.wikipedia.org/wiki/Data_Plane_Development_Kit), It seems to be a network card driver and some libraries. It works based on polling instead of interrupts. [Seastar](http://seastar.io/) makes use of it. The fact that we are in need of DPDK, shows that the Linux Kernel is not fast enough for some needs, for special cases using userland stack is fine, but kernel should be fast enough for other tasks.

Applying the [end-to-end principle](/docs/notes/study/internet#end-to-end-principle) here, pushing as much work as possible out of the core kernel and toward the actual applications. It's faster if the data reaches the CPU running the application by facing minimum locking and shared data of the underlying network stack.

### Playing with the Linux Kernel Network Stack

- It can be agreed that 50kpps per core is probably the limit for any practical application. but what about the Linux Network stack?
- Measuring `pps` is more interesting than measureing `Bps`.
- for this experiment must ensure the traffic won't be interfered with by the iptables.
- it uses sendmmsg and recvmmsg instead of send and recv syscall because of efficiency. c programs were written for sender and reciver.
  https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=a2e2725541fad72416326798c2d7fa4dafb7d337
- uses the taskset command to Pinning the processes to CPUs so that kernel not shuffling our programs between cores. improved processor cache locality.
  MORE packets
- ethtool was used to determine where the packets went.
  rx_nodesc_drop_cnt shows how many packets are dropped by the NIC.

Example shows muliqueue, my laptop NIC does not support multique apparently.

- promblem with single RX queue was that impossible to deliver more packets than a single CPU could handle. To support multicore systems, NICs started supporting multiple RX queues.
- ethtool also allows choosing the multiqueue hashing algorithm
- NUMA performance with reciever program running on one CPU and the RX Queue recieving on other CPU, when run cross NUMA nodes, penalty is ~10% but variablity was bad.

RECIEVING in multiple IPS
two cores busy with handling RX queues, and third running the application, it's possible to get more speed.

if we keep on increasing this number of increasing the number of RX quque we are able to deliver through, we hit another limit that can be seen through netstat, the , this time we dont get rx_nodesc_drop_cnt but RcvbufErrors can be seen with netstat, (netstat no longer there!) this means This means that while the NIC is able to deliver the packets to the kernel, the kernel is not able to deliver the packets to the application.

> nstat is a tool to monitor kernel snmp counters and network interface statistics.

https://blog.cloudflare.com/how-to-achieve-low-latency/
https://blog.cloudflare.com/kernel-bypass/
https://blog.cloudflare.com/how-to-receive-a-million-packets/

### @bwploktas post on memory usage monitoring in Go1.12

üîç [Link to post](https://www.bwplotka.dev/2019/golang-memory-monitoring/)

- Basically [Go1.12 started using `MADV_FREE`](https://github.com/golang/go/issues/23687#issuecomment-496705293) instead of `MADV_DONTNEED` with [`madvise`](http://man7.org/linux/man-pages/man2/madvise.2.html) which means that the running application will just mark the memory as **available for other to use** but will not release the memory immediately. This is useful and prevents wasted work releasing the memory for others to use in container environments where mostly there's only one process running in a container. But _does create some problem related to how monitoring applications should see memory now._
- Due to `MAD_FREE`, `container_memory_rss` and `container_memory_usage_bytes` show higher memory usage even if the program is not using that much memory.
- We could use `go_memstats_.*_inuse_.*`, `go_memstats_alloc_bytes` etc metrics to see **actual/accurate** allocations, but in-use memory does **NOT** include `mmap` files and memory allocated by CGO.
- The current recommendation is using `container_memory_working_set_bytes`(WSS) metric, which is just `total_inactive_file` subtracted from `container_memory_usage_bytes`
- If you want to know how much memory your Go program is really using, you need to carefully look at the various bits and pieces in `runtime.MemStats`, perhaps exported through `net/http/pprof`.

### Twitch Story: How I learnt to stop worrying and love the heap

üîç [Link to post](https://blog.twitch.tv/en/2019/04/10/go-memory-ballast-how-i-learnt-to-stop-worrying-and-love-the-heap-26c2462549a2/)

- They were running a service called `visage` which was their API frontend, upon inspecting with [pprof](/docs/notes/study/go_lang/pprof) they found that, at steady state the app was triggering ~8‚Äì10 GC-cycles/s or ~600/m and 30% of CPU cycles were being spent in function calls related to GC!
- The Go pacer in their case was doing a superb job of keeping garbage on our heap to a minimum, but it was coming at the cost of unnecessary work, since we were only using ~0.6% of our system‚Äôs memory (The system had 64GiB memory and `visage` was only using like 400MiB).
- They could have fine tuned the `GOGC` variable to make the pacer trigger at slower pace but instead decided to use **"heap ballast"** to fix this problem, basically assigning a big allocation to the running application primarily because ballasts are easier to reason about.
- `ballast := make([]byte, 10<<30)`: Create a large heap allocation of 10 GiB; Using `byte` array enables the GC to mark the entire object in O(1) time.
  > Just in case `10<<30` means `10*2^30` since `2^30 ~= 1bn` and 1bn bytes = 1GB
  >
  > **Doubt**: Does this not mean `gc pause` will only occur when `VSZ` crosses `20GiB`, i.e the double. If the case is that 20GiB is never reached, `gc pause` is never run? (Maybe gc cycle a better word as Go GC is concurrent)
- Memory in ‚Äònix (and even Windows) systems is virtually addressed and mapped through _page tables_ by the OS. The array the ballast slice points to will be allocated in the program‚Äôs virtual address space(VAS). So, only if we attempt to read or write to the slice, will the page fault occur that causes the physical RAM backing the virtual addresses to be allocated. The post shows a nice example of this comparing `Resident Set Size(RSS)` and `Virtual SiZe (VSZ)` : This basically means, **just** allocating the `10GiB` ballast will **not** actually takeup `10GiB` in the physical RAM.
- They also saw **improvement** in _API latency_, as a result of the GC running less frequently; but this actually happened due to the fact that the Mutator(our application) now has to do no `MARK ASSIST` work when GC cycle is not running. So by simply reducing GC frequency, they saw close to a ~99% drop in mark assist work.
- Finally offers some nice thoughts on why `MARK ASSIST` is a good idea in a concurrent GC.
